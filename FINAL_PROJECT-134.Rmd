---
title: "PSTAT 134 - Final Project"
author: "Jonathan Palada Rosal, Dana Lee"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
fontfamily: palatino
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(skimr)
library(patchwork)
library(janitor)
library(vembedr)
library(ISLR)
library(ISLR2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(readr)
library(corrplot)
library(tune)
library(ggplot2)
library(tree)
library(rpart)
library(rpart.plot)
library("ggpubr")
library(glmnet)
library(xgboost)
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
library(MLmetrics)
library(vip)
```

## Introduction
Laptop computers are useful electronic devices that are primarily used for both entertainment and work purposes. A laptop allows you to take your work with you wherever you go, yet it may come with unnecessary features that not everyone may require. For this project, we aimed to create an optimal model that would allow us to forecast a price for a laptop that only includes the features that the customer desires. 

### Our Dataset
The data we will be using for this project is laptop specs and their prices. (https://www.kaggle.com/datasets/kuchhbhi/latest-laptop-price-list) Our dataset is from Kaggle with 23 variables and 1,000 observations.The dataset contains many variables, but we will focus on these 16 factors to make the prediction: Processor Brand, Processor Name, Processor Generation, Ram (GB), Ram Type, SSD (GB), HDD (GB), Operating System, Os_bit, Graphic Card (GB), Weight, Display Size, Warranty, Touchscreen, MSOffice, and Latest Price.
```{r, echo=FALSE}
embed_youtube("bMDSikB6ooE")
```

### How our model can be helpful
A college student with an essay due every week will undoubtedly require a laptop with different features than a software developer who writes code every day. This model can be useful in assisting customers in selecting the features they want in their laptops and determining estimated prices based on what they require. They can also use the model to see what options are available if they have a specific budget in mind for their laptop.

In order for our model to be helpful, we need it to be accurate. Therefore, our goal will be to get a model that has an accuracy of over 70%. If we accomplish this we believe that our model will be accurate enough to produce helpful predictions to consumers.

### Reading/Cleaning the dataset

```{r}
laptop <- read.csv("Cleaned_Laptop_data.csv")
laptop <- laptop %>%
  clean_names()
laptop <- laptop[-19:-23] # removed the variables not needed for the study
laptop <- laptop[-1:-2] # removed the brand and model variable
```

Removed the variables that are not needed for the study. 

```{r}
laptop <- subset(laptop, processor_brand != "Missing")
laptop <- subset(laptop, processor_name != "Missing") 
laptop <- subset(laptop, processor_gnrtn != "Missing") 
laptop <- subset(laptop, ram_gb != "Missing") 
laptop <- subset(laptop, ram_type != "Missing") 
laptop <- subset(laptop, ssd != "Missing") 
laptop <- subset(laptop, hdd != "Missing") 
laptop <- subset(laptop, os != "Missing") 
laptop <- subset(laptop, os_bit != "Missing") 
laptop <- subset(laptop, graphic_card_gb != "Missing") 
laptop <- subset(laptop, weight != "Missing") 
laptop <- subset(laptop, display_size != "Missing") 
laptop <- subset(laptop, warranty != "Missing") 
laptop <- subset(laptop, touchscreen != "Missing") 
laptop <- subset(laptop, msoffice != "Missing") 
laptop <- subset(laptop, latest_price != "Missing") # 415 observations now
laptop$display_size <- as.numeric(laptop$display_size)
laptop$latest_price <- laptop$latest_price/81.41 # Turn the currency into U.S. dollars
```

Deleted any rows with missing information. Also converted the price variable into U.S. dollars.

```{r}
table(laptop$ram_gb) 
laptop['ram_gb'][laptop['ram_gb'] == '16 GB GB'] <- 16
laptop['ram_gb'][laptop['ram_gb'] == '32 GB GB'] <- 32
laptop['ram_gb'][laptop['ram_gb'] == '4 GB GB'] <- 4
laptop['ram_gb'][laptop['ram_gb'] == '8 GB GB'] <- 8
laptop$ram_gb <- as.numeric(laptop$ram_gb)
```

```{r}
table(laptop$ssd)
laptop['ssd'][laptop['ssd'] == '0 GB'] <- 0
laptop['ssd'][laptop['ssd'] == '1024 GB'] <- 1024
laptop['ssd'][laptop['ssd'] == '128 GB'] <- 128
laptop['ssd'][laptop['ssd'] == '2048 GB'] <- 2048
laptop['ssd'][laptop['ssd'] == '256 GB'] <- 256
laptop['ssd'][laptop['ssd'] == '32 GB'] <- 32
laptop['ssd'][laptop['ssd'] == '512 GB'] <- 512
laptop$ssd <- as.numeric(laptop$ssd)
```

```{r}
table(laptop$hdd)
laptop['hdd'][laptop['hdd'] == '0 GB'] <- 0
laptop['hdd'][laptop['hdd'] == '1024 GB'] <- 1024
laptop['hdd'][laptop['hdd'] == '2048 GB'] <- 2048
laptop['hdd'][laptop['hdd'] == '512 GB'] <- 512
laptop$hdd <- as.numeric(laptop$hdd)
```

We made the variables that had numbers as values into actual numerical data.

### An overview of the dataset

```{r, echo=FALSE}
head(laptop)
```

These are the variables we will be using in our dataset:

```processor_brand``` : Processor Brand

```processor_name``` : Processor Name

```processor_gnrtn``` : Processor Generation

```ram_gb``` : Random Access Memory in GB

```ram_type``` : Random Access Memory type

```ssd``` : Solid State Drive in GB

```hdd``` : Hard Disk Drive in GB

```os``` : Operating system

```os_bit``` : Operating system bit

```graphic_card_gb``` : Graphic Card in GB

```weight``` : Weight of laptop in categories of: Casual, ThinNlight, Gaming

```display_size``` : Size in inches

```Warranty``` : Years of warranty after purchase

```touchscreen``` : If it has a touchscreen feature

```msoffice``` : If it has Microsoft office

```{r, echo=FALSE}
summary(laptop)
```

After using the read.csv function to read the dataset, we cleaned the data by removing observations containing missing values. We also removed two variables, brand and model, that were not required for our model, leaving a total of 415 observations with 16 variables. When looking at the summary we can conclude that most our variables are categorical.

## Exploratory Data Analysis

### Display Size

![Fig 1. Size differences between laptops](images/laptop-screen-size-comparison.jpg){width="500"}

Display size is one of the most important features that most consumers consider. The display size's range from 12.2 (inches) - 17.3 (inches). Some consumers may want a 12.2 for easier transportation. Other consumers may want a 17.3 for a bigger screen to see more details in visualizations. On average most students pick a display size between 13 and 15 inches. The perfect in between for students who need easy transport and a decently size screen.

```{r, echo=FALSE}
plot(laptop$display_size,laptop$latest_price, pch = 19, col = "lightblue", xlab="Display size", ylab = "Latest Price ", main = "Correlation between Display Size and Latest Price")
# Regression line
abline(lm(laptop$display_size ~ laptop$latest_price), col = "red", lwd = 3)
# Pearson correlation
text(paste("Correlation:", round(cor(laptop$display_size, laptop$latest_price), 2)), x = 12.75, y = 4000)
```

Looking at the plot we can see that there is a 20% positive correlation between the price of the laptop and the display size. Therefore, if a consumer wants a bigger screen, you will most likely pay more for that bigger screen.

### Price

![Fig 2. Amazon: Laptop Pricing](images/Pricing.jpeg){width="500"}

When purchasing laptops, one of the most important factors customers consider is price. The laptops users require vary greatly depending on their purpose and budget, and navigating the manufacturers' websites to find the model that perfectly fits can be overwhelming. Therefore, we decided to use the price as our responsive variable in this project in order to focus on selecting the best laptop.

```{r, echo=FALSE}
ggplot(laptop, aes(latest_price)) +
   geom_histogram(bins = 60, color = "red") +
   labs(title = "Range of Laptop Prices")
```

According to the plot, the majority of laptops in our dataset are priced between \$250 and \$4300. Majority of the laptops range from \$400 - \$1250. A reason why most laptops are priced within this range is because this range is the typical budget for most buyers.  The higher-priced laptops are probably larger and heavier and have far more sophisticated features.


### Processor

![Fig 3. Intel Processor](images/Processor.jpg){width="300"}

The key element that customers most closely associate with high-performing, quick technology is the computer processor. When comparing computers, the most important factor to consider is the computer processor speed. Making sure the processor works properly is critical to the longevity and functionality of each laptop. Our processor variable contains CPU's, GPU's, APU's and Graphic Cards. 

```CPU``` - Designed for general purpose tasks, most common applications, and is very compute intensive

```GPU``` - Designed for specialized tasks, such as graphics and video, most visual applications, and data processing in parallel.

```APU``` - Combining CPU & GPU elements into a single architecture to form a single chip. As a result they consume less power than a CPU + GPU resulting in a better battery life for the laptop. 

```Graphic Card``` - Contains one or two GPUs, a cooling system, dedicated RAM, and a dedicated source of power. Graphic cards are designed for being in charge of generating images and videos to display then on an output device, such as a monitor/tv.


```{r, echo=FALSE}
laptop %>%
  ggplot(aes(reorder(processor_name, latest_price), latest_price)) +  geom_boxplot(varwidth = TRUE) + coord_flip() + labs(subtitle = "Processors",x = NULL)
```

Observing the box plot created above, the top 3 processors with the highest prices are GeForce RTX, Core i9, and M1. These are the top-ranked processors that are known for their high-speed, and video gamers who require advanced programs use them to deliver realistic graphics with incredibly fast performance or cutting-edge new AI features like NVIDIA DLSS and NVIDIA Broadcast. A customer with a lower budget should purchase a laptop with an APU since it can give a great price to performance ratio. Based on this plot we can see that the faster the processor, the more expensive it is.

### Features Significantly Impacting Price

![Fig 4. Specifications](images/Significant factors.jpg){width="350"}

We have many different variables in our dataset. On top of those variables, there are many different options that a laptop can have. With so many different options, you can make a huge amount of possible combinations with so many options. Some of these options could increase the price significantly. Therefore we will make a model that can list which features significantly impact the price.

```{r, echo=FALSE}
model <- lm(latest_price ~ ., data = laptop)
summary(model)
```

Going by a significance level of 0.05, the following features have a significantly impact the price:

```Processor Brand``` : AMD, Intel

```Processor Name``` : The type of processor is significant to the price

```Processor Generation``` : 8th generation is significant to the price

```RAM GB``` : RAM is significant to the price

```RAM Type``` : DDR3

```SSD GB``` : SSD is significant to the price

```HHD GB``` : 512

```Operating System``` : The Operating System is significant to the price.

```Weight``` : The weight is significant to the price.

```Display Size``` : All sizes are significant to the price

```Touchscreen``` : Having a touchscreen is significant to the price.

If a consumer was considering any of these features, they will now know that these features could drastically increase or decrease the price of the laptop. These features could be considered a decision-maker for some consumers.

## Preparation for modeling

The following models conducted were done in this order and procedure:
1. Building the model
2. Running the model
3. Making predictions using the model

### Preparing the data

```{r}
par(mfrow=c(2,2))
laptopbc <- laptop
hist(laptop$latest_price, col="light blue", xlab = "Price", main="Original Data")
bcTransform <- boxcox(laptop$latest_price~ ., data = laptop) 
lambda=bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
laptopbc$latest_price <- (1/lambda)*(laptop$latest_price^lambda-1)
hist(laptopbc$latest_price, col="light blue", xlab="Price", main="Box-Cox transformation")  #looks the most symmetric and normally distributed
```

We transformed the `laptop_price` variable by performing a Box-Cox transformation. This transformation should make the data look more like a normal distribution. Looking at the 95% confidence interval for the true $\lambda$, we could see that the suggested transformations were $Y_{t} = \frac{1}{\lambda} (X_{t}^{\lambda} - 1)$. When doing a Box-Cox transformation we get a more normal like- distribution.

```{r}
laptop <- laptop %>%
  mutate(processor_brand = factor(processor_brand)) %>%
  mutate(processor_name = factor(processor_name)) %>%
  mutate(processor_gnrtn = factor(processor_gnrtn)) %>%
  mutate(ram_type  = factor(ram_type)) %>%
  mutate(os = factor(os)) %>%
  mutate(os_bit = factor(os_bit)) %>%
  mutate(weight = factor(weight)) %>%
  mutate(touchscreen = factor(touchscreen)) %>%
  mutate(msoffice = factor(msoffice))
head(laptop)

laptopbc <- laptopbc %>%
  mutate(processor_brand = factor(processor_brand)) %>%
  mutate(processor_name = factor(processor_name)) %>%
  mutate(processor_gnrtn = factor(processor_gnrtn)) %>%
  mutate(ram_type  = factor(ram_type)) %>%
  mutate(os = factor(os)) %>%
  mutate(os_bit = factor(os_bit)) %>%
  mutate(weight = factor(weight)) %>%
  mutate(touchscreen = factor(touchscreen)) %>%
  mutate(msoffice = factor(msoffice))
head(laptopbc)
```
We mutated the variables by factoring the numerical predictors.

### Splitting the data

```{r}
rownames(laptop) <- 1:nrow(laptop) #updating index numbers
laptop

rownames(laptopbc) <- 1:nrow(laptopbc) # updating index numbers
laptopbc

set.seed(12)
ec_split <- laptop %>%
  initial_split(prop = 0.80, strata = "latest_price")

ec_train <- training(ec_split)
ec_test <- testing(ec_split)
dim(ec_train) #331 obs. 16 columns
dim(ec_test) #84 obs. 16 columns

ec_trainbc <- ec_train
ec_trainbc$latest_price <- (1/lambda)*(ec_trainbc$latest_price^lambda-1)
ec_testbc <- ec_test
ec_testbc$latest_price <- (1/lambda)*(ec_testbc$latest_price^lambda-1)
```
We split the data into  80% training and 20% testing as we felt that would be the best way to approach training and testing our models. We had to make two different splits. For ridge and lasso, the engines we use for those models already find the most optimal lambda and perform a transformation while predicting. For the boost and tree models we had to perform the transformations manually.


### Making the recipe and folds

```{r}
ec_recipe <- recipe(latest_price ~ processor_brand + processor_name + processor_gnrtn + ram_gb + ram_type + ssd + hdd + os + os_bit + graphic_card_gb + weight + display_size + warranty + touchscreen + msoffice, data = ec_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_novel(all_nominal_predictors())
               

ec_folds <- vfold_cv(ec_train, strata = latest_price, v = 10, repeats = 5)

ec_recipebc <- recipe(latest_price ~ processor_brand + processor_name + processor_gnrtn + ram_gb + ram_type + ssd + hdd + os + os_bit + graphic_card_gb + weight + display_size + warranty + touchscreen + msoffice, data = ec_trainbc) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_zv(all_nominal_predictors())
               

ec_foldsbc <- vfold_cv(ec_trainbc, strata = latest_price, v = 10, repeats = 5)
```

We made a recipe using the training set. The predictor variables we left out of the recipe are brand and model. We decided to leave them out because we wanted the prediction to be completely based on the features of the laptop. We `step_dummy()`  all nominal predictors to encode them as categorical predictors. We also `step_normalize()` to center and scale all the predictors. We `step_novel` and `step_zv` all nominal predictors so it would assign any previously unseen factor level to a new value and to remove any variables that contain only a single value.

## The models

### Ridge Regression

The first model we decided to create was a Ridge Regression Model. Ridge regression is one of the alternative approaches to modeling. Ridge is one of the main types of the Regularization approach. The goal of the Regularization approach is to shrink the coefficient estimates toward zero, similar to least squares. Ridge minimizes the sum of squared residuals and $\lambda * slope^2$.

```{r, message=FALSE, warning=FALSE}
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

ridge_workflow <- workflow() %>%
  add_recipe(ec_recipe) %>%
  add_model(ridge_spec)

set.seed(12)

penalty_grid <- grid_regular(penalty(range = c(1, 11)), levels = 50)
penalty_grid

tune_res <- tune_grid(
  ridge_workflow,
  resamples = ec_folds,
  grid = penalty_grid
)
tune_res

autoplot(tune_res)
```

In this step we are adding the recipe to the ridge model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. We are using the glmnet engine so it will perform the most optimal transformation for the model.

```{r}
Ridge_RMSE <- collect_metrics(tune_res) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_penalty <- select_best(tune_res, metric = "rsq")
best_penalty

ridge_final <- finalize_workflow(ridge_workflow, best_penalty)
ridge_final_fit <- fit(ridge_final, data = ec_train)

Ridge_Prediction <- predict(ridge_final_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Ridge_Prediction <- bind_cols(Ridge_Prediction, ec_test %>% dplyr::select(latest_price))
Ridge_Prediction <- (1/lambda)*(Ridge_Prediction^lambda-1) #To make the values the same as the boost and tree models


Ridge_Graph <- Ridge_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha = 1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Ridge_Accuracy <- augment(ridge_final_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end. We converted the predictions and actual values into the same scale as the boxcox data so it will be easier to compare to the boost and tree models.


### Lasso Regression

The second model we decided to create was a Lasso Regression Model. Lasso regression is also one of the alternative approaches to modeling. Like Ridge, Lasso is one of the main types of the Regularization approach.The difference is Lasso minimizes the sum of squared residuals and $\lambda * |slope|$.

```{r, message=FALSE, warning=FALSE}
lasso_spec <-
  linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

lasso_workflow <- workflow() %>%
  add_recipe(ec_recipe) %>%
  add_model(lasso_spec)

set.seed(12)

tune_res_lasso <- tune_grid(
  lasso_workflow,
  resamples = ec_folds,
  grid = penalty_grid
)
tune_res_lasso

autoplot(tune_res_lasso)
```

In this step we are adding the recipe to the lasso model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The plots seem to increase/decrease quickly compared to the Ridge Model. We are also using the glmnet engine here as well.

```{r}
Lasso_RMSE <- collect_metrics(tune_res_lasso) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_penalty_lasso <- select_best(tune_res_lasso, metric = "rsq")

lasso_final <- finalize_workflow(lasso_workflow, best_penalty_lasso)
lasso_final_fit <- fit(lasso_final, data = ec_train)

Lasso_Prediction <- predict(lasso_final_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Lasso_Prediction <- bind_cols(Lasso_Prediction, ec_test %>% dplyr::select(latest_price))
Lasso_Prediction <- (1/lambda)*(Lasso_Prediction^lambda-1) #To make the values the same as the boost and tree models

Lasso_Graph <- Lasso_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Lasso_Accuracy <- augment(lasso_final_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end. We also converted the predictions and actual values into the same scale as the boxcox data so it will be easier to compare to the boost and tree models.

### Boosted Model

The third model we created was a boosted tree model. A boosted model builds a weak decision tree that has low predictive accuracy. Then the model goes through the process of sequentially improving previous decision trees. Doing this, slowly reduces the bias at each step without drastically increasing the variance.
```{r, message=FALSE, warning=FALSE}
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_wf <- workflow() %>%
  add_model(boost_spec %>%
  set_args(trees = tune())) %>%
  add_recipe(ec_recipebc)

set.seed(12)

boost_grid <- grid_regular(trees(range = c(10, 2000)), levels = 50)

boost_tune_res <- tune_grid(
  boost_wf,
  resamples = ec_foldsbc,
  grid = boost_grid,
)

autoplot(boost_tune_res)
```

In this step we are adding the recipe to the Boost model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The boost rmse plot decreases significantly but then levels out around the 60th tree. The rsq does the exact same thing but increases instead.

```{r}
Boost_RMSE <- collect_metrics(boost_tune_res) %>% 
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_boost_final <- select_best(boost_tune_res)
best_boost_final_model <- finalize_workflow(boost_wf, best_boost_final)
best_boost_final_model_fit <- fit(best_boost_final_model, data = ec_trainbc)

Boost_Prediction <- predict(best_boost_final_model_fit, new_data = ec_testbc %>% dplyr::select(-latest_price))
Boost_Prediction <- bind_cols(Boost_Prediction, ec_testbc %>% dplyr::select(latest_price))

Boost_Graph <- Boost_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Boost_Accuracy <- augment(best_boost_final_model_fit, new_data = ec_testbc) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.

### Decision - Tree model

The fourth and final model we decided to make is a decision tree model. A decision tree model puts the data into classified chunks. Then based on the data from those chunks, the model does it's best to predict the outcome.
```{r, message=FALSE, warning=FALSE}
tree_spec <-decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("regression")
  
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(ec_recipebc)

set.seed(12)

param_grid <- grid_regular(cost_complexity(range = c(-5, 5)), levels = 50)

tune_res_tree <- tune_grid(
  class_tree_wf,
  resamples = ec_foldsbc,
  grid = param_grid,
)

autoplot(tune_res_tree)
```

In this step we are adding the recipe to the Tree model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The `cost_complexity` parameter seems to have more similarities to the ridge plots. The difference is the plots are not smooth, and seem to have sudden changes of slope.

```{r}
Tree_RMSE <- collect_metrics(tune_res_tree) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_complexity <- select_best(tune_res_tree)
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = ec_trainbc)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

The tree plot asks specific questions. These questions can only be answered yes or no. 
```{r}
Tree_Prediction <- predict(class_tree_final_fit, new_data = ec_testbc %>% dplyr::select(-latest_price))
Tree_Prediction <- bind_cols(Tree_Prediction, ec_testbc %>% dplyr::select(latest_price))

Tree_Graph <- Tree_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Tree_Accuracy <- augment(class_tree_final_fit, new_data = ec_testbc) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.

## Results

Comparison of the four different models: 
We will compare the four different models in this by these factors:
- Prediction Graphs
- RSQ (R-Squared) from Training Set
- RSQ from Testing Set

### Graphs

```{r, echo=FALSE}
figure <- ggarrange(Ridge_Graph, Lasso_Graph, Boost_Graph,Tree_Graph,
                    labels = c("Ridge", "Lasso", "Boost","Tree"),
                    ncol = 2, nrow = 2)
figure
```

In the plots the dotted line represents where the points would be if the actual price of the laptop was the same number as the prediction. Looking at the plots I would say that the Boost has the points closest to the dotted line meaning they most likely have the highest accuracy between the four models.

### RSQ (Training Set)

Ridge
```{r, echo=FALSE}
head(Ridge_RMSE)
```

Looking at the mean and standard we get that 
Ridge has the following values:
```RSQ``` : mean = 0.753 & standard error = 0.0145

Lasso
```{r, echo=FALSE}
head(Lasso_RMSE)
```
Looking at the mean and standard we get that 
Lasso has the following values:
```RSQ``` : mean = 0.752 & standard error = 0.0137

Boost
```{r, echo=FALSE}
head(Boost_RMSE)
```
Looking at the mean and standard we get that 
Boost has the following values:
```RSQ``` : mean = 0.884 & standard error = 0.00978

Tree
```{r, echo=FALSE}
head(Tree_RMSE)
```
Looking at the mean and standard we get that 
Tree has the following values:
```RSQ``` : mean = 0.748 & standard error = 0.0205

Looking at all the model's rsq, Boost would be the best model to test on the Testing Set. Boost has the highest RSQ mean and the lowest standard error.

### R-Squared for (Testing Set)

```{r, echo=FALSE}
rsq_comparisons <- bind_rows(Ridge_Accuracy, Lasso_Accuracy, Boost_Accuracy, Tree_Accuracy) %>% 
  tibble() %>% mutate(model = c("Ridge", "Lasso", "Boost", "Tree")) %>% 
  dplyr::select(model, .estimate) %>%
  arrange(.estimate)

rsq_comparisons
```

We combine the predictions of our model with the actual values of the testing set. After doing that we can see the R-Squared value which will tell us the accuracy of the models we used. Looking at the R-Squared of the four different models we see that the Boost model had the highest R-Squared and the tree model had the lowest R-Squared.

## Conclusions:

Exploring the dataset we found some variables that significantly that affect the price of the laptop. After we decided to produce four different models to predict the price. These models were: Ridge, Lasso, Boost, and Tree.

Before analyzing the results of the rsq from the training set, we thought the tree model would do best because most of our data was categorical. Based on the results, we can say that the tree model performed well. However, boost came out on top as the highest accuracy. Boost came out on top because it reduced the bias the best without significantly increasing the variance. The lasso and ridge models performed worse than the tree and boost models.

This research shows Boost would be the best model. This model is the best at predicting the price of a laptop, based on the features the laptop has. Predicting with this model, will give the customer a price with around 85% accuracy. The goal of our project was achieved because we got a model with an accuracy over 70%.

Although our tree model didn't have the best accuracy, it can be the most useful model because it offers a solution for every distinct criterion that must be considered when looking for the best laptop. Customers can use the tree model to determine what features to look for based on the yes/no responses for each step of the tree, making it convenient to identify all required features. The tree model is simple to understand and to interpret, and the results are straightforward. Since the tree model can be used for both classification and regression models and can accommodate both categorical and numerical data, it can be the most useful model for our original dataset, which contains both categorical and numerical variables. 

In conclusion, the boost model performed with the highest accuracy of 84.9%, but we would recommend our tree model. The tree model does not have the highest accuracy, however it is still over 70% and meets our goal. We think that the usefulness of the tree model overshadows the decrease in accuracy. 

### What we improved on?

After talking to the professor during our presentation he recommended some changes we should do to improve our project. One tweak we made was to change all our GB variables into numerical. Before the change, our GB variables were categorical. In order to do this change we deleted the GB part in the values then used the `as.numeric()` function to convert it to numeric. 

The second change we made was to box-cox our response variable. Before, only lasso and ridge were being transformed because of the glmnet engine we used for those models. However, the boost and tree models were using the original data. So we transformed the data for the boost and tree models in order to get a higher accuracy model. 

After doing both of these changes are boost and tree model's accuracy increased significantly while our lasso and ridge model's accuracy slightly decreased.

### What can still be improved?

An improvement that can be made for this project is to replace the missing details for most of the laptops. After cleaning the data we lost more than half of the data. If that missing data was filled with updated information then we could have used all of the data. Using the whole dataset would of most likely improved the accuracy of the model because it would of had more observations to train on.

