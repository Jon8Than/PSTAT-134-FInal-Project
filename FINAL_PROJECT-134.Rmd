---
title: "PSTAT 134 - Final Project"
author: "Jonathan Palada Rosal, Dana Lee"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
  pdf_document:
    latex_engine: pdflatex
fontfamily: palatino
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(skimr)
library(patchwork)
library(janitor)
library(vembedr)
library(ISLR)
library(ISLR2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(readr)
library(corrplot)
library(tune)
library(ggplot2)
library(tree)
library(rpart)
library(rpart.plot)
library("ggpubr")
library(glmnet)
library(xgboost)
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
library(MLmetrics)
library(vip)
```

## Introduction
Laptop computers are useful electronic devices that are primarily used for both entertainment and work purposes.A laptop allows you to take your work with you wherever you go, yet it may come with unnecessary features that not everyone may require. For this project, we aimed to create an optimal model that would allow us to forecast a price for a laptop that only includes the features that the customer desires. 

### (our topic)
The data we will be using for this project is laptop specs and their prices. (https://www.kaggle.com/datasets/kuchhbhi/latest-laptop-price-list) Our dataset is from Kaggle with 23 variables and 1,000 observations.The dataset contains many variables, but we will focus on these 16 factors to make the prediction: Processor Brand, Processor Name, Processor Generation, Ram (GB), Ram Type, SSD (GB), HDD (GB), Operating System, Os_bit, Graphic Card (GB), Weight, Display Size, Warranty, Touchscreen, MSOffice, and Latest Price.

### How my model can be helpful
A college student with an essay due every week will undoubtedly require a laptop with different features than a software developer who writes code every day. This model can be useful in assisting customers in selecting the features they want in their laptops and determining estimated prices based on what they require. They can also use the model to see what options are available if they have a specific budget in mind for their laptop.

### Reading in the dataset

```{r}
laptop <- read.csv("Cleaned_Laptop_data.csv")
head(laptop)
laptop <- laptop %>%
  clean_names()
laptop <- laptop[-19:-23] # removed the variables not needed for the study
laptop <- laptop[-1:-2] # removed the brand and model variable
```

```{r}
#laptop <- subset(laptop, brand != "Missing") 
#laptop <- subset(laptop, model != "Missing") 
laptop <- subset(laptop, processor_brand != "Missing")
laptop <- subset(laptop, processor_name != "Missing") 
laptop <- subset(laptop, processor_gnrtn != "Missing") 
laptop <- subset(laptop, ram_gb != "Missing") 
laptop <- subset(laptop, ram_type != "Missing") 
laptop <- subset(laptop, ssd != "Missing") 
laptop <- subset(laptop, hdd != "Missing") 
laptop <- subset(laptop, os != "Missing") 
laptop <- subset(laptop, os_bit != "Missing") 
laptop <- subset(laptop, graphic_card_gb != "Missing") 
laptop <- subset(laptop, weight != "Missing") 
laptop <- subset(laptop, display_size != "Missing") 
laptop <- subset(laptop, warranty != "Missing") 
laptop <- subset(laptop, touchscreen != "Missing") 
laptop <- subset(laptop, msoffice != "Missing") 
laptop <- subset(laptop, latest_price != "Missing") # 415 observations now
laptop$display_size <- as.numeric(laptop$display_size)
```

```{r}
summary(laptop)
```

### An overview of the dataset
After using the read.csv function to read the dataset, we cleaned the data by removing observations containing missing values. We also removed two variables, brand and model, that were not required for our model, leaving a total of 415 observations with 16 variables.

## Exploratory Data Analysis

### Correlation between Display Size and Laptop Price
```{r}
plot(laptop$display_size,laptop$latest_price, pch = 19, col = "lightblue", xlab="Display size", ylab = "Latest Price ", main = "Correlation between Display Size and Latest Price")
# Regression line
abline(lm(laptop$display_size ~ laptop$latest_price), col = "red", lwd = 3)
# Pearson correlation
text(paste("Correlation:", round(cor(laptop$display_size, laptop$latest_price), 2)), x = 14, y = 300000)
```

EXPLANATION?????????



### Price
When purchasing laptops, one of the most important factors customers consider is price. The laptops users require vary greatly depending on their purpose and budget, and navigating the manufacturers' websites to find the model that perfectly fits can be overwhelming. Therefore, we decided to use the price as our responsive variable in this project in order to focus on selecting the best laptop.

```{r}
ggplot(laptop, aes(latest_price)) +
   geom_histogram(bins = 60, color = "red") +
   labs(title = "Range of Laptop Prices") +
   xlim(20000, 400000)
```
According to the plot, the majority of laptops in our dataset are priced between $20,000 and $100,000. The higher-priced laptops are probably larger and heavier and have far more sophisticated features.


### Processor
The key element that customers most closely associate with high-performing, quick technology is the computer processor. When comparing computers, the most important factor to consider is the computer processor speed. Making sure the processor works properly is critical to the longevity and functionality of each laptop.

```{r}
laptop %>%
  ggplot(aes(reorder(processor_name, latest_price), latest_price)) +  geom_boxplot(varwidth = TRUE) + coord_flip() + labs(subtitle = "Processors",x = NULL)
```
Observing the box plot created above, the top 3 processors with the highest prices are GeForce RTX, Core i9, and M1. These are the top-ranked processors that are known for their high-speed, and video gamers who require advanced programs use them to deliver realistic graphics with incredibly fast performance or cutting-edge new AI features like NVIDIA DLSS and NVIDIA Broadcast.

### Significant Features

```{r}
model <- lm(latest_price ~ ., data = laptop)
summary(model)
```

The following features have a significant 

## Preparation for modeling

The following models conducted were done in this order and procedure:
1. Building the model
2. Running the model
3. Making predictions using the model

### Preparing the data

```{r}
laptop <- laptop %>%
 # mutate(brand = factor(brand)) %>%
  #mutate(model = factor(model)) %>%
  mutate(processor_brand = factor(processor_brand)) %>%
  mutate(processor_name = factor(processor_name)) %>%
  mutate(processor_gnrtn = factor(processor_gnrtn)) %>%
  mutate(ram_gb = factor(ram_gb)) %>%
  mutate(ram_type  = factor(ram_type)) %>%
  mutate(ssd = factor(ssd)) %>%
  mutate(hdd = factor(hdd)) %>%
  mutate(os = factor(os)) %>%
  mutate(os_bit = factor(os_bit)) %>%
  mutate(weight = factor(weight)) %>%
  mutate(display_size = factor(display_size)) %>%
  mutate(touchscreen = factor(touchscreen)) %>%
  mutate(msoffice = factor(msoffice))
head(laptop)
```
I mutated the variables by factoring the numerical predictors.

### Splitting the data

```{r}
set.seed(12)
ec_split <- laptop %>%
  initial_split(prop = 0.80, strata = "latest_price")

ec_train <- training(ec_split)
dim(ec_train) #331 obs. 18 columns
ec_test <- testing(ec_split)
dim(ec_test) #84 obs. 18 columns
```
We split the data into a traditional 80% training and 20% testing as we felt that would be the best way to approach training and testing our models.


### Making the recipe and folds

```{r}
ec_recipe <- recipe(latest_price ~ processor_brand + processor_name + processor_gnrtn + ram_gb + ram_type + ssd + hdd + os + os_bit + graphic_card_gb + weight + display_size + warranty + touchscreen + msoffice, data = ec_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_zv(all_nominal_predictors())
               

ec_folds <- vfold_cv(ec_train, strata = latest_price, v = 10, repeats = 5)
```

We made a recipe using the training set. The predictor variables we left out of the recipe are brand and model. I decided to leave them out because I wanted the prediction to be completely based on the features of the laptop. I `step_dummy()`  all nominal predictors to encode them as categorical predictors. I also `step_normalize()` to center and scale all the predictors. I `step_novel` and `step_zv` all nominal predictors so it would assign any previously unseen factor level to a new value and to remove any variables that contain only a single value.

## The models

### Ridge Regression

The first model I decided to create was a Ridge Regression Model. Ridge regression is one of the alternative approaches to modeling. Ridge is one of the main types of the Regularization approach. The goal of the Regularization approach is to shrink the coefficient estimates toward zero, similar to least squares. Ridge minimizes the sum of squared residuals and $\lambda * slope^2$.

```{r, message=FALSE, warning=FALSE}
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

ridge_workflow <- workflow() %>%
  add_recipe(ec_recipe) %>%
  add_model(ridge_spec)

set.seed(12)

penalty_grid <- grid_regular(penalty(range = c(1, 11)), levels = 50)
penalty_grid

tune_res <- tune_grid(
  ridge_workflow,
  resamples = ec_folds,
  grid = penalty_grid
)
tune_res

autoplot(tune_res)
```

In this step we are adding the recipe to the ridge model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also.

```{r}
Ridge_RMSE <- collect_metrics(tune_res) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_penalty <- select_best(tune_res, metric = "rsq")
best_penalty

ridge_final <- finalize_workflow(ridge_workflow, best_penalty)
ridge_final_fit <- fit(ridge_final, data = ec_train)

Ridge_Prediction <- predict(ridge_final_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Ridge_Prediction <- bind_cols(Ridge_Prediction, ec_test %>% dplyr::select(latest_price))

Ridge_Graph <- Ridge_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha = 1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Ridge_Accuracy <- augment(ridge_final_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.


### Lasso Regression

The second model I decided to create was a Lasso Regression Model. Lasso regression is also one of the alternative approaches to modeling. Like Ridge, Lasso is one of the main types of the Regularization approach.The difference is Lasso minimizes the sum of squared residuals and $\lambda * |slope|$.

```{r, message=FALSE, warning=FALSE}
lasso_spec <-
  linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

lasso_workflow <- workflow() %>%
  add_recipe(ec_recipe) %>%
  add_model(lasso_spec)

set.seed(12)

tune_res_lasso <- tune_grid(
  lasso_workflow,
  resamples = ec_folds,
  grid = penalty_grid
)
tune_res_lasso

autoplot(tune_res_lasso)
```

In this step we are adding the recipe to the lasso model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The plots does not seem to be as smooth as the ridge model.

```{r}
Lasso_RMSE <- collect_metrics(tune_res_lasso) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_penalty_lasso <- select_best(tune_res_lasso, metric = "rsq")

lasso_final <- finalize_workflow(lasso_workflow, best_penalty_lasso)
lasso_final_fit <- fit(lasso_final, data = ec_train)

Lasso_Prediction <- predict(lasso_final_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Lasso_Prediction <- bind_cols(Lasso_Prediction, ec_test %>% dplyr::select(latest_price))

Lasso_Graph <- Lasso_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Lasso_Accuracy <- augment(lasso_final_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.

### Boosted Model

The third model I created was a boosted tree model. A boosted model builds a weak decision tree that has low predictive accuracy. Then the model goes through the process of sequentially improving previous decision trees. Doing this, slowly reduces the bias at each step without drastically increasing the variance.
```{r, message=FALSE, warning=FALSE}
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_wf <- workflow() %>%
  add_model(boost_spec %>%
  set_args(trees = tune())) %>%
  add_recipe(ec_recipe)

set.seed(12)

boost_grid <- grid_regular(trees(range = c(10, 2000)), levels = 50)

boost_tune_res <- tune_grid(
  boost_wf,
  resamples = ec_folds,
  grid = boost_grid,
)

autoplot(boost_tune_res)
```

In this step we are adding the recipe to the Boost model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The tree plots here seem to go straight up and then flatten out, never changing it's slope again.


```{r}
Boost_RMSE <- collect_metrics(boost_tune_res) %>% 
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_boost_final <- select_best(boost_tune_res)
best_boost_final_model <- finalize_workflow(boost_wf, best_boost_final)
best_boost_final_model_fit <- fit(best_boost_final_model, data = ec_train)

Boost_Prediction <- predict(best_boost_final_model_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Boost_Prediction <- bind_cols(Boost_Prediction, ec_test %>% dplyr::select(latest_price))

Boost_Graph <- Boost_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Boost_Accuracy <- augment(best_boost_final_model_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.

### Decision - Tree model

The fourth and final model I decided to make is a decision tree model. A decision tree model puts the data into classified chunks. Then based on the data from those chunks, the model does it's best to predict the outcome.
```{r, message=FALSE, warning=FALSE}
tree_spec <-decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("regression")
  
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(ec_recipe)

set.seed(12)

param_grid <- grid_regular(cost_complexity(range = c(-5, 5)), levels = 50)

tune_res_tree <- tune_grid(
  class_tree_wf,
  resamples = ec_folds,
  grid = param_grid,
)

autoplot(tune_res_tree)
```

In this step we are adding the recipe to the Tree model. We are also making the workflow and grid for the `tune_grid`. We use the folds we did earlier for the `tune_grid` also. The `cost_complexity` parameter seems to have more similarities to the lasso plots. The reasoning is the plots are not smooth, and seem to have sudden changes of slope.

```{r}
Tree_RMSE <- collect_metrics(tune_res_tree) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

We collect the metrics of our regression tune and look at the mean and standard error.

```{r}
best_complexity <- select_best(tune_res_tree)
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = ec_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

The tree plot asks specific questions. These questions can only be answered yes or no. 

```{r}
Tree_Prediction <- predict(class_tree_final_fit, new_data = ec_test %>% dplyr::select(-latest_price))
Tree_Prediction <- bind_cols(Tree_Prediction, ec_test %>% dplyr::select(latest_price))

Tree_Graph <- Tree_Prediction %>%
  ggplot(aes(x=.pred, y=latest_price)) + geom_point(alpha=1) + geom_abline(lty = 2) + theme_bw() + coord_obs_pred()

Tree_Accuracy <- augment(class_tree_final_fit, new_data = ec_test) %>%
  rsq(truth = latest_price, estimate = .pred)
```

Here we prepare the predictions, graphs, and plots, for comparison at the end.

## Conclusions

Comparison of the four different models: 
We will compare the four different models in this by these factors:
- Prediction Graphs
- RMSE & RSQ (R-Squared) from Training Set
- RSQ from Testing Set

### Graphs

```{r}
figure <- ggarrange(Ridge_Graph, Lasso_Graph, Boost_Graph,Tree_Graph,
                    labels = c("Ridge", "Lasso", "Boost","Tree"),
                    ncol = 2, nrow = 2)
figure
```

In the plots the dotted line represents where the points would be if the actual price of the vehicle was the same number as the prediction. Looking at the plots I would say that the Boost has the points closest to the dotted line meaning they most likely have the highest accuracy between the four models.

### RMSE & RSQ (Training Set)

Ridge
```{r}
head(Ridge_RMSE)
```

Looking at the mean and standard we get that 
Ridge has the following values:
```RMSE``` : mean = 18,922 & standard error = 2013
```RSQ``` : mean = 0.7994 & standard error = 0.03249

Lasso
```{r}
head(Lasso_RMSE)
```
Looking at the mean and standard we get that 
Lasso has the following values:
```RMSE``` : mean = 20,886 & standard error = 2544
```RSQ``` : mean = 0.715 & standard error = 0.04805

Boost
```{r}
head(Boost_RMSE)
```
Looking at the mean and standard we get that 
Boost has the following values:
```RMSE``` : mean = 23,443 & standard error = 2127
```RSQ``` : mean = 0.8232 & standard error = 0.04482

Tree
```{r}
head(Tree_RMSE)
```
Looking at the mean and standard we get that 
Tree has the following values:
```RMSE``` : mean = 25,233 & standard error = 2102
```RSQ``` : mean = 0.584 & standard error = 0.0399

Looking at all the model's rmse and rsq, Boost would be the best model to test on the Testing Set. Boost has the lowest RMSE and highest RSQ.

### R-Squared for (Testing Set)

```{r}
rsq_comparisons <- bind_rows(Ridge_Accuracy, Lasso_Accuracy, Boost_Accuracy, Tree_Accuracy) %>% 
  tibble() %>% mutate(model = c("Ridge", "Lasso", "Boost", "Tree")) %>% 
  dplyr::select(model, .estimate) %>%
  arrange(.estimate)

rsq_comparisons
```

Looking at the R-Squared of the four different models we see that the Boost model had the highest R-Squared and the tree model had the lowest R-Squared.

### What can be improved?

\

